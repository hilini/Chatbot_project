#!/usr/bin/env python3
"""
vLLM ì„ë² ë”© ì„œë²„ í´ë¼ì´ì–¸íŠ¸
HTTP APIë¥¼ í†µí•´ vLLM ì„ë² ë”© ì„œë²„ì™€ í†µì‹ 
"""

import aiohttp
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)

class VLLMEmbeddingClient:
    """vLLM ì„ë² ë”© ì„œë²„ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, server_url: str = "http://localhost:8002", model_name: str = "bge-large"):
        """
        Args:
            server_url: vLLM ì„ë² ë”© ì„œë²„ URL (ê¸°ë³¸ í¬íŠ¸ 8002)
            model_name: ì‚¬ìš©í•  ëª¨ë¸ëª… (OpenAI í˜¸í™˜ ëª¨ë¸ ì´ë¦„)
        """
        self.server_url = server_url.rstrip('/')
        self.model_name = model_name
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def health_check(self) -> Dict[str, Any]:
        """ì„œë²„ ìƒíƒœ í™•ì¸: GET /v1/models í˜¸ì¶œë¡œ ëŒ€ì²´"""
        try:
            url = f"{self.server_url}/v1/models"
            async with self.session.get(url) as response:
                if response.status == 200:
                    return {"status": "healthy"}
                else:
                    raise Exception(f"Health check failed: {response.status}")
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return {"status": "unhealthy", "error": str(e)}
    
    async def create_embeddings(self, texts: List[str], normalize: bool = True) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (OpenAI í˜¸í™˜ API)
        
        Args:
            texts: ë³€í™˜í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            normalize: L2 ì •ê·œí™” ì—¬ë¶€ (í´ë¼ì´ì–¸íŠ¸ ì¸¡)
            
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return []
        
        try:
            payload = {
                "model": self.model_name,
                "input": texts
            }
            
            url = f"{self.server_url}/v1/embeddings"
            async with self.session.post(
                url,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status != 200:
                    text = await response.text()
                    raise Exception(f"Embedding request failed: {response.status} - {text}")
                
                data = await response.json()
                embs = [item["embedding"] for item in data.get("data", [])]
                
                if normalize:
                    # í´ë¼ì´ì–¸íŠ¸ ì¸¡ L2 ì •ê·œí™”
                    embs = [
                        (np.array(v) / np.linalg.norm(v)).tolist()
                        if np.linalg.norm(v) > 0 else v
                        for v in embs
                    ]
                
                return embs
                    
        except Exception as e:
            logger.error(f"Embedding creation failed: {e}")
            # fallback: ëœë¤ ì„ë² ë”© (768 ì°¨ì›)
            dim = 1024 if "large" in self.model_name else 768
            return [np.random.randn(dim).tolist() for _ in texts]
    
    async def create_single_embedding(self, text: str, normalize: bool = True) -> List[float]:
        embeddings = await self.create_embeddings([text], normalize)
        return embeddings[0] if embeddings else []
    
    async def similarity(self, text1: str, text2: str) -> float:
        try:
            embs = await self.create_embeddings([text1, text2], normalize=True)
            if len(embs) == 2:
                a, b = np.array(embs[0]), np.array(embs[1])
                return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))
            return 0.0
        except Exception as e:
            logger.error(f"Similarity calculation failed: {e}")
            return 0.0
    
    async def load_model(self, model_name: str) -> bool:
        """(vLLM ì „ìš©) ì„œë²„ì— ë‹¤ë¥¸ ëª¨ë¸ ë¡œë“œ ìš”ì²­ (/load_model ì—”ë“œí¬ì¸íŠ¸)"""
        try:
            payload = {"model_name": model_name}
            async with self.session.post(f"{self.server_url}/load_model", json=payload) as response:
                if response.status == 200:
                    self.model_name = model_name
                    return True
                return False
        except Exception as e:
            logger.error(f"Model loading failed: {e}")
            return False
    
    async def get_available_models(self) -> Dict[str, str]:
        """(vLLM ì „ìš©) /models ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            async with self.session.get(f"{self.server_url}/models") as response:
                if response.status == 200:
                    res = await response.json()
                    return res.get("available_models", {})
                return {}
        except Exception as e:
            logger.error(f"Model list retrieval failed: {e}")
            return {}

# ë™ê¸° ë˜í¼ í´ë˜ìŠ¤ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´)
class VLLMEmbeddingManager:
    """ë™ê¸° ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” vLLM ì„ë² ë”© ë§¤ë‹ˆì €"""
    
    def __init__(self, server_url: str = "http://localhost:8002", model_name: str = "bge-large"):
        self.server_url = server_url
        self.model_name = model_name
    
    def encode(self, texts: List[str], normalize: bool = True) -> np.ndarray:
        async def _encode():
            async with VLLMEmbeddingClient(self.server_url, self.model_name) as client:
                embs = await client.create_embeddings(texts, normalize)
                return np.array(embs)
        return asyncio.run(_encode())
    
    def encode_single(self, text: str, normalize: bool = True) -> np.ndarray:
        return self.encode([text], normalize)[0]
    
    def similarity(self, text1: str, text2: str) -> float:
        async def _sim():
            async with VLLMEmbeddingClient(self.server_url, self.model_name) as client:
                return await client.similarity(text1, text2)
        return asyncio.run(_sim())
    
    def get_embedding_dimension(self) -> int:
        if "large" in self.model_name:
            return 1024
        elif "base" in self.model_name:
            return 768
        elif "small" in self.model_name:
            return 384
        else:
            return 768

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    async def test_vllm_client():
        print("ğŸ§ª vLLM ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        async with VLLMEmbeddingClient() as client:
            health = await client.health_check()
            print(f"ì„œë²„ ìƒíƒœ: {health}")
            
            if health.get("status") == "healthy":
                test_texts = [
                    "í¨ë¸Œë¡¤ë¦¬ì£¼ë§™ì€ ë©´ì—­í•­ì•”ì œì…ë‹ˆë‹¤.",
                    "í‚¤íŠ¸ë£¨ë‹¤ëŠ” íì•” ì¹˜ë£Œì— ì‚¬ìš©ë˜ëŠ” ë©´ì—­í•­ì•”ì œì…ë‹ˆë‹¤.",
                    "ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤."
                ]
                
                print("ì„ë² ë”© ìƒì„± ì¤‘...")
                embeddings = await client.create_embeddings(test_texts)
                print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ, ì°¨ì›: {len(embeddings[0])}")
                
                sim1 = await client.similarity(test_texts[0], test_texts[1])
                print(f"ì˜ë£Œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {sim1:.4f}")
                
                sim2 = await client.similarity(test_texts[0], test_texts[2])
                print(f"ë¬´ê´€ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {sim2:.4f}")
                
                models = await client.get_available_models()
                print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(models.keys())}")
            else:
                print("âŒ ì„œë²„ê°€ ì •ìƒ ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")
    
    asyncio.run(test_vllm_client()) 