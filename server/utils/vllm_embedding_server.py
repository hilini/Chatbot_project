#!/usr/bin/env python3
"""
vLLM ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸ ì„œë¹™ ì„œë²„
GPU ê°€ì† ì„ë² ë”© ìƒì„± ë° ì„œë¹™
"""

import asyncio
import json
import logging
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import numpy as np

# vLLM ì„í¬íŠ¸
try:
    from vllm import LLM, SamplingParams
    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    from vllm.sampling_params import SamplingParams
    from vllm.utils import random_uuid
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    print("vLLMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install vllmë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

# FastAPI ì•± ìƒì„±
app = FastAPI(title="vLLM Embedding Server", version="1.0.0")

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class EmbeddingRequest(BaseModel):
    texts: List[str]
    model_name: str = "BAAI/bge-large-en-v1.5"
    normalize: bool = True

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]]
    model_name: str
    embedding_dim: int
    text_count: int

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    available_models: List[str]

# ì „ì—­ ë³€ìˆ˜
embedding_engine = None
current_model = None

# ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ë“¤
AVAILABLE_MODELS = {
    "bge-large": "BAAI/bge-large-en-v1.5",
    "bge-base": "BAAI/bge-base-en-v1.5", 
    "bge-small": "BAAI/bge-small-en-v1.5",
    "e5-large": "intfloat/e5-large-v2",
    "e5-base": "intfloat/e5-base-v2",
    "e5-small": "intfloat/e5-small-v2",
    "minilm": "sentence-transformers/all-MiniLM-L6-v2",
    "mpnet": "sentence-transformers/all-mpnet-base-v2"
}

async def load_embedding_model(model_name: str):
    """vLLM ì—”ì§„ìœ¼ë¡œ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    global embedding_engine, current_model
    
    if not VLLM_AVAILABLE:
        raise RuntimeError("vLLMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # ì‹¤ì œ ëª¨ë¸ëª… í™•ì¸
        actual_model = AVAILABLE_MODELS.get(model_name, model_name)
        
        print(f"vLLM ì—”ì§„ ì´ˆê¸°í™” ì¤‘: {actual_model}")
        
        # vLLM ì—”ì§„ ì„¤ì •
        engine_args = AsyncEngineArgs(
            model=actual_model,
            trust_remote_code=True,
            max_num_batched_tokens=4096,
            max_num_seqs=256,
            gpu_memory_utilization=0.8,
            tensor_parallel_size=1,  # ë‹¨ì¼ GPU ì‚¬ìš©
            dtype="float16",  # ë©”ëª¨ë¦¬ ì ˆì•½
            enforce_eager=True,  # ë””ë²„ê¹…ìš©
        )
        
        # ë¹„ë™ê¸° ì—”ì§„ ìƒì„±
        embedding_engine = AsyncLLMEngine.from_engine_args(engine_args)
        current_model = model_name
        
        print(f"âœ… vLLM ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ: {actual_model}")
        return True
        
    except Exception as e:
        print(f"âŒ vLLM ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

async def generate_embeddings(texts: List[str], normalize: bool = True) -> List[List[float]]:
    """vLLMì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±"""
    if not embedding_engine:
        raise RuntimeError("ì„ë² ë”© ì—”ì§„ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
        # BGE ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì‚¬ìš©
        if "bge" in current_model.lower():
            prompts = [f"Represent this sentence for searching relevant passages: {text}" for text in texts]
        else:
            prompts = texts
        
        # vLLMìœ¼ë¡œ ì„ë² ë”© ìƒì„±
        sampling_params = SamplingParams(
            temperature=0.0,  # ê²°ì •ì  ì¶œë ¥
            max_tokens=1,  # ì„ë² ë”©ë§Œ í•„ìš”
            stop=None
        )
        
        # ë¹„ë™ê¸°ë¡œ ì„ë² ë”© ìƒì„±
        results = []
        for prompt in prompts:
            request_id = random_uuid()
            result_generator = embedding_engine.generate(prompt, sampling_params, request_id)
            
            async for result in result_generator:
                # ë§ˆì§€ë§‰ hidden stateë¥¼ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©
                if result.outputs:
                    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í† í° ì„ë² ë”©ì˜ í‰ê· ì„ ì‚¬ìš©
                    # ì‹¤ì œë¡œëŠ” ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ ë°©ë²•ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
                    embeddings = result.outputs[0].token_embeddings
                    if embeddings:
                        embedding = np.mean(embeddings, axis=0).tolist()
                        if normalize:
                            # L2 ì •ê·œí™”
                            norm = np.linalg.norm(embedding)
                            if norm > 0:
                                embedding = (np.array(embedding) / norm).tolist()
                        results.append(embedding)
                    else:
                        # fallback: ëœë¤ ì„ë² ë”©
                        results.append(np.random.randn(768).tolist())
        
        return results
        
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        # fallback: ëœë¤ ì„ë² ë”©
        return [np.random.randn(768).tolist() for _ in texts]

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ"""
    print("ğŸš€ vLLM ì„ë² ë”© ì„œë²„ ì‹œì‘ ì¤‘...")
    
    # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
    success = await load_embedding_model("bge-large")
    if not success:
        print("âš ï¸ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ì„œë²„ëŠ” ì‹œì‘ë˜ì§€ë§Œ ì„ë² ë”© ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return HealthResponse(
        status="healthy",
        model_loaded=embedding_engine is not None,
        available_models=list(AVAILABLE_MODELS.keys())
    )

@app.post("/embed", response_model=EmbeddingResponse)
async def create_embeddings(request: EmbeddingRequest):
    """ì„ë² ë”© ìƒì„± API"""
    try:
        # ëª¨ë¸ì´ ë³€ê²½ëœ ê²½ìš° ìƒˆë¡œ ë¡œë“œ
        if request.model_name != current_model:
            success = await load_embedding_model(request.model_name)
            if not success:
                raise HTTPException(status_code=500, detail="ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        
        # ì„ë² ë”© ìƒì„±
        embeddings = await generate_embeddings(request.texts, request.normalize)
        
        return EmbeddingResponse(
            embeddings=embeddings,
            model_name=current_model,
            embedding_dim=len(embeddings[0]) if embeddings else 0,
            text_count=len(embeddings)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.get("/models")
async def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return {
        "available_models": AVAILABLE_MODELS,
        "current_model": current_model
    }

@app.post("/load_model")
async def load_model(model_name: str):
    """ëª¨ë¸ ë¡œë“œ API"""
    try:
        success = await load_embedding_model(model_name)
        if success:
            return {"status": "success", "model": model_name}
        else:
            raise HTTPException(status_code=500, detail="ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    print("ğŸ”§ vLLM ì„ë² ë”© ì„œë²„ ì‹œì‘...")
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
    for key, model in AVAILABLE_MODELS.items():
        print(f"   - {key}: {model}")
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8001,
        log_level="info"
    ) 